{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "3c8de8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2379d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [r\"Quiz1.html\",r\"Quiz2.html\",r\"Quiz3.html\",r\"Quiz4.html\",r\"Quiz5.html\",r\"Quiz6.html\",r\"Quiz7.html\",r\"Quiz8.html\",\n",
    "        r\"Quiz9.html\",r\"Quiz10.html\",r\"Quiz11.html\",r\"Quiz12.html\",r\"Quiz13.html\",r\"Quiz14.html\",r\"Quiz15.html\",r\"Quiz16.html\",\n",
    "        r\"Quiz17.html\",r\"Quiz18.html\",r\"Quiz19.html\",r\"Quiz20.html\",r\"Quiz21.html\",r\"Quiz22.html\",r\"Quiz23.html\",r\"Quiz24.html\",\n",
    "        r\"Quiz25.html\",r\"Quiz26.html\",r\"Quiz27.html\",r\"Quiz28.html\",r\"Quiz29.html\",r\"Quiz30.html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5f257f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(soup):\n",
    "    mydivs = soup.find_all(\"div\", {\"class\": \"rc-FormPartsQuestion\"})\n",
    "    for i,div in enumerate(mydivs):\n",
    "        question = div.find('div',{\"data-test\": 'legend'})\n",
    "        answers = div.find_all('div',{\"class\":\"rc-Option rc-Option--isReadOnly\"})\n",
    "        for q in question.select('p'):\n",
    "            print(f\"{q.text.capitalize()}\")\n",
    "        for j,answer in enumerate(answers):\n",
    "            print(f\"{chr(j+65)}: \",end=\"\")\n",
    "            for line in answer.select('p'):\n",
    "                print(line.text.capitalize())\n",
    "            if 'RadioChecked' in answer.find(\"svg\")[\"aria-labelledby\"]:\n",
    "                correct = answer.select('p')\n",
    "        print(\"---\")\n",
    "        for c in correct:\n",
    "            print(c.text.capitalize())\n",
    "        print(\"</>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4568120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer describes an \n",
      "A: Troubleshooting problem\n",
      "B: Set of inputs\n",
      "C: Algorithm\n",
      "D: Computer program\n",
      "---\n",
      "Algorithm\n",
      "</>\n",
      "To make this algorithm functional, which step would you add to step 4?\n",
      "Scan to find the smallest number\n",
      "Set to 0 in the index in the output array\n",
      "Remove that number from the input array \n",
      "__________________________________________\n",
      "A: Repeat steps 1-3, but subtract the total from the numbers summated\n",
      "B: Print output in the correct order\n",
      "C: Divide the array by the index, print the array output\n",
      "D: Repeat steps 1-3, but add 1 to the index number for each loop \n",
      "---\n",
      "Repeat steps 1-3, but add 1 to the index number for each loop \n",
      "</>\n",
      "Take input, get output, use output on next input is an example of a learning algorithm\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "Learning algorithms require large datasets, which means storing identifying information about users\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "A _________ _________ in machine learning is the idea that the algorithm itself influences the next set of inputs that go into the model. the main takeaway is that algorithms sometimes have more influence than a user's initial input.\n",
      "A: Feedback loop\n",
      "B: Large dataset\n",
      "C: Learning algorithm\n",
      "D: False dichotomy\n",
      "---\n",
      "Feedback loop\n",
      "</>\n",
      "Are anonymous datasets truly anonymous? \n",
      "A: Yes, thanks to privacy regulations\n",
      "B: Yes, thanks to machine learning randomness algorithms\n",
      "C: No, due to lacking regulations\n",
      "D: No, due to combining data and re-identification\n",
      "---\n",
      "No, due to combining data and re-identification\n",
      "</>\n",
      "Which of the following best describes what an algorithm is?\n",
      "A: A type of computer that calculates problem-solving methods\n",
      "B: A type of process a human uses to write down what steps need to happen to get a problem solved\n",
      "C: A recipe that a computer uses to solve problems\n",
      "D: A list of ingredients a computer uses to generate problems to solve\n",
      "---\n",
      "A recipe that a computer uses to solve problems\n",
      "</>\n",
      "An algorithm that takes an input, tries 10 different sorting techniques, and uses the best fit on the next 100 inputs is best described as a\n",
      "A: Learning algorithm\n",
      "B: Explicit algorithm\n",
      "C: Data algorithm\n",
      "D: Implicit algorithm\n",
      "---\n",
      "Learning algorithm\n",
      "</>\n",
      "Which of these steps follows the most logical order for a low-to-high sorting algorithm?\n",
      "A: Scan to find the smallest number\n",
      "\n",
      "Set to 0 in the index in the output array\n",
      "\n",
      "Remove that number from the input array\n",
      "\n",
      "Repeat steps 1-3, but add 1 to the index number for each loop\n",
      "\n",
      "B: Scan to find the smallest number\n",
      "\n",
      "Set the length of the array in the index in the output array\n",
      "\n",
      "Remove that number from the input\n",
      "\n",
      "Repeat steps 1-3, but add 1 to the index number for each loop\n",
      "\n",
      "C: Scan to find the smallest number\n",
      "Set to 0 in the index in the output array\n",
      "Remove that number from the input array\n",
      "D: Scan to find the largest number\n",
      "Set to 0 in the index in the output array\n",
      "Remove that number from the input array\n",
      "Repeat steps 1-3, but add 1 to the index number for each loop \n",
      "---\n",
      "Scan to find the smallest number\n",
      "\n",
      "Set to 0 in the index in the output array\n",
      "\n",
      "Remove that number from the input array\n",
      "\n",
      "Repeat steps 1-3, but add 1 to the index number for each loop\n",
      "\n",
      "</>\n",
      "What's the difference between a basic and learning algorithm?\n",
      "A: A basic algorithm takes an input and gets an output, while a learning algorithm takes multiple inputs and gets multiple outputs\n",
      "B: An basic algorithm takes an input and gets an output, while a learning algorithm uses the output on the next input\n",
      "C: A basic algorithm takes an input, gets an output, while a learning algorithm takes multiple inputs and gets multiple outputs\n",
      "D: A basic algorithm takes an input, while a learning algorithm takes an input and gets an output\n",
      "---\n",
      "An basic algorithm takes an input and gets an output, while a learning algorithm uses the output on the next input\n",
      "</>\n",
      "Pseudocode can best be defined as \n",
      "A: A python library for machine learning\n",
      "B: A middle ground between code and plain writing that can be fed into a computer\n",
      "C: A type of javascript that is both human and machine-readable\n",
      "D: An explainable description of code that is meant for humans, not computers\n",
      "---\n",
      "An explainable description of code that is meant for humans, not computers\n",
      "</>\n",
      "What side effect of learning algorithms creates an ethical dilemma for its users?\n",
      "A: Learning algorithms require government regulation, which is bad for software developers\n",
      "B: Learning algorithms are costly to run, which drives up prices for consumer services\n",
      "C: Learning algorithms require large datasets, which means storing identifying information about users\n",
      "D: Learning algorithms require large amounts of computing power, which is bad for the environment\n",
      "---\n",
      "Learning algorithms require large datasets, which means storing identifying information about users\n",
      "</>\n",
      "How do anonymized datasets fall short of their goal of being anonymous?\n",
      "A: Anonymized datasets can be combined with other datasets, which can re-identify individuals\n",
      "B: Anonymized datasets can be re-identifyed by anyone holding the hash key\n",
      "C: Anonymized datasets can be traced back to the individuals by looking at their browsing history in the app\n",
      "D: Anonymized datasets aren't actually anonymous because many of the data fields can identify a user\n",
      "---\n",
      "Anonymized datasets can be combined with other datasets, which can re-identify individuals\n",
      "</>\n",
      "What is a likely outcome for a weather app using a learning algorithm to figure out where to put their future weather stations? \n",
      "A: Accessing weather forecasts from local broadcasts\n",
      "B: Storing data in an aws instance with all weather stations in the country\n",
      "C: Collecting location data every time the app is opened, potentially learning where a user lives, works, etc. \n",
      "D: Collecting weather data every time the app is opened, knowing the temperature where the app is being used\n",
      "---\n",
      "Collecting location data every time the app is opened, potentially learning where a user lives, works, etc. \n",
      "</>\n",
      "Which of the following is a good  example of a feedback loop in machine learning?\n",
      "A: A social media site tracks engagement, uses an algorithm to surface posts you're likely to engage with, which then goes back into the algorithm\n",
      "B: A social media site surfaces controversial posts, which make users more angry and lead to more angry posts on the network\n",
      "C: A shopping app tracks your purchases, and recommends new things to buy\n",
      "D: A shopping app surfaces new items to buy, which is based on dataset from customers fitting a similar profile. when you buy, you go into that dataset\n",
      "---\n",
      "A social media site tracks engagement, uses an algorithm to surface posts you're likely to engage with, which then goes back into the algorithm\n",
      "</>\n",
      "A fact of learning algorithms is that \n",
      "A: Even if you haven't shared an direct datapoint about yourself, with enough related datapoints the algorithm can make an educated guess with alarming accuracy \n",
      "B: Just because they are capable of improving outputs, they don't need more inputs\n",
      "C: They only learn when given small amounts of data, and without the proper training sample, the results can be wildly inaccurate\n",
      "D: They cannot make predictions with current technology\n",
      "---\n",
      "Even if you haven't shared an direct datapoint about yourself, with enough related datapoints the algorithm can make an educated guess with alarming accuracy \n",
      "</>\n",
      "A basic learning model can figure out which of the 10 sorting mechanisms works best for this type of input. a complex model ______________________\n",
      "A: Automatically scans all inputs\n",
      "B: Can figure out up to 150 mechanisms\n",
      "C: Can figure out up to 50 mechanisms\n",
      "D: Automatically derives its mechanism\n",
      "---\n",
      "Automatically derives its mechanism\n",
      "</>\n",
      "A model's error rate is the ratio of incorrect predictions to total predictions\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "The goal of the _______________ is to get the model's error rate as low as possible. to do this, we repeat a cycle of feeding training data, compare predictions to actual outcomes, and adjust the model as needed.\n",
      "A: Develop phase\n",
      "B: Algorithmic phase\n",
      "C: Training phase\n",
      "D: Deployment phase\n",
      "---\n",
      "Training phase\n",
      "</>\n",
      "As models become more complex, researchers are unable to reason why the decisions are being made. this is called the _________________ \n",
      "A: Rationality phase\n",
      "B: Black box problem\n",
      "C: Training phase\n",
      "D: False input problem\n",
      "---\n",
      "Black box problem\n",
      "</>\n",
      "____________ can often be caused by predicting what someone may or may not do based on data \n",
      "A: Irrationality\n",
      "B: Dataset collection\n",
      "C: Real harm\n",
      "D: Retraining scenarios\n",
      "---\n",
      "Real harm\n",
      "</>\n",
      "Hedge funds largely rely on predictive models to judge the movement of stocks, bonds, and securities\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "What's the difference between a basic and complex learning algorithm?\n",
      "A: A basic algorithm cannot use computer vision, while a complex algorithm can \n",
      "B: A basic algorithm cannot process more than 5 steps in a function, while a complex algorithm can process up to 15\n",
      "C: A basic algorithm has a set amount of choices to optimize for, while a complex algorithm is given the freedom to find its own model\n",
      "D: A basic algorithm can handle simple inputs like numbers, while a complex algorithm can handle complex inputs like pictures\n",
      "---\n",
      "A basic algorithm has a set amount of choices to optimize for, while a complex algorithm is given the freedom to find its own model\n",
      "</>\n",
      "When building a predictive model, what is the goal of the develop phase?\n",
      "A: To get the model to accept new inputs, train, and repeat training until it finds a better curve\n",
      "B: To specify the type of algorithm the model should use and make sure the data is cleaned/formatted\n",
      "C: To plug in 40% of your dataset, testing the model's accuracy\n",
      "D: To get the model's error function below an acceptable percentage\n",
      "---\n",
      "To specify the type of algorithm the model should use and make sure the data is cleaned/formatted\n",
      "</>\n",
      "When building a predictive model, what is the goal of the training phase?\n",
      "A: To adjust training methods from backpropagation to supervised learning to see how that affects outputs\n",
      "B: To use the model in real-world scenarios, monitoring performance\n",
      "C: To adjust the model based on a subset of data, optimizing for a lower error rate\n",
      "D: To specify the type of algorithm the model should use and make sure the data is cleaned/formatted\n",
      "---\n",
      "To adjust the model based on a subset of data, optimizing for a lower error rate\n",
      "</>\n",
      "When building a predictive model, what is the goal of the deployment phase?\n",
      "A: To specify the type of algorithm the model should use and make sure the data is cleaned/formatted\n",
      "B: To plug in 40% of your dataset, testing the model's accuracy\n",
      "C: To use the model in real-life predictions, monitoring the error rate and accuracy \n",
      "D: To get the model to accept new inputs, train, and repeat training until it finds a better curve\n",
      "---\n",
      "To use the model in real-life predictions, monitoring the error rate and accuracy \n",
      "</>\n",
      "What are the attributes of an error function when training a predictive model\n",
      "A: The percentage of data that is formatted properly \n",
      "B: The ratio of training data to actual data the model has consumed\n",
      "C: The ratio of algorithm to curve in a predictive model\n",
      "D: The percentage of predictions that don't match actual outcomes\n",
      "---\n",
      "The percentage of predictions that don't match actual outcomes\n",
      "</>\n",
      "In a complex learning function, we will understand the ____, but not the ____\n",
      "A: Causal link, correlation\n",
      "B: Algorithm, cause and effect\n",
      "C: Inputs/outputs, algorithm\n",
      "D: Input data, output data\n",
      "---\n",
      "Inputs/outputs, algorithm\n",
      "</>\n",
      "What is the black box problem?\n",
      "A: The problem created when researchers don't create accurate attributes for a model\n",
      "B: When a model cannot accurately judge shape or color of objects due to missing data\n",
      "C: The issue of not having enough data to accurately train a model\n",
      "D: When a model is deployed, but researchers are unable to figure out why it's making decisions\n",
      "---\n",
      "When a model is deployed, but researchers are unable to figure out why it's making decisions\n",
      "</>\n",
      "Which of the following is a negative consequence of a predictive model used in real life?\n",
      "A: A model used by a lab wrongly predicts a person will not be able to pay their credit card\n",
      "B: A model used by a lab indicates a person is in danger\n",
      "C: A model used by a bank accurately predicts a person will not be able to pay off a loan\n",
      "D: A model used by a bank wrongly predicts a person will not be able to pay off a loan\n",
      "---\n",
      "A model used by a bank wrongly predicts a person will not be able to pay off a loan\n",
      "</>\n",
      "How are predictive models used in hedge funds?\n",
      "A: They aid researchers by forecasting financial collapse\n",
      "B: They provide predictions to shareholders to estimate returns\n",
      "C: They predict future movement of stocks and find points to exploit the market moving in either direction\n",
      "D: They predict whether people will be able to pay off loans, and then provide loans\n",
      "---\n",
      "They predict future movement of stocks and find points to exploit the market moving in either direction\n",
      "</>\n",
      "What is one possible reason a model may predict a higher crime rate based on datasets used?\n",
      "A: If a dataset isn't properly formatted, crime may be linked to the error function, outputting false data\n",
      "B: If crime is down in an area, a model may predict a parabolic curve which estimates crime is due to rise again\n",
      "C: The model's training curve was not provided enough data\n",
      "D: If drug arrests are historically high in that area, the model may correlate crime with areas of high drug use based on the datasets\n",
      "---\n",
      "If drug arrests are historically high in that area, the model may correlate crime with areas of high drug use based on the datasets\n",
      "</>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To measure accuracy, take the number of ________ results and divide over the number of all results \n",
      "A: False positive and false negative\n",
      "B: True positive and true negative\n",
      "C: False positive and true negative\n",
      "D: True positive and false negative\n",
      "---\n",
      "True positive and true negative\n",
      "</>\n",
      "A false negative result is one in which the model predicts a result was negative, and in reality it was ___________. it is an _________ prediction\n",
      "A: Positive, correct\n",
      "B: Negative, correct\n",
      "C: Positive, incorrect\n",
      "D: Negative, incorrect\n",
      "---\n",
      "Positive, incorrect\n",
      "</>\n",
      "City and state are correlated data, but a model will measure no variation and the results will not be affected\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "A ________ training set relies on running a final accuracy test before deploying a model. an __________ training set relies on multiple tests to ensure that a model is free of bias\n",
      "A: Sample, optimized\n",
      "B: Test, classic\n",
      "C: Optimized, classic\n",
      "D: Classic, optimized\n",
      "---\n",
      "Classic, optimized\n",
      "</>\n",
      "An unknown unknown is an example of a cultural reflection of data\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "An ethical predictive model needs to be accurate, _____________, and fair \n",
      "A: Moral\n",
      "B: Predictable\n",
      "C: Truthful\n",
      "D: Explainable\n",
      "---\n",
      "Explainable\n",
      "</>\n",
      "To measure a predictive model's accuracy, you \n",
      "A: Multiply the number of total predictions by the percentage of correct predictions\n",
      "B: Divide the number of predictions by the total dataset\n",
      "C: Measure the ratio of the model's error curve\n",
      "D: Divide the number of correct predictions by the total number of predictions\n",
      "---\n",
      "Divide the number of correct predictions by the total number of predictions\n",
      "</>\n",
      "A predictive model's false negative result can be defined as \n",
      "A: The predicted result was positive, and the actual result was positive\n",
      "B: The predicted result was positive, and the actual result was negative\n",
      "C: The predicted result was negative, and the actual result was positive\n",
      "D: The predicted result was negative, and the actual result was negative\n",
      "---\n",
      "The predicted result was negative, and the actual result was positive\n",
      "</>\n",
      "A predictive model's true positive result can be defined as \n",
      "A: The predicted result was positive, and the actual result was positive\n",
      "B: The predicted result was negative, and the actual result was negative\n",
      "C: The predicted result was negative, and the actual result was positive\n",
      "D: The predicted result was positive, and the actual result was negative\n",
      "---\n",
      "The predicted result was positive, and the actual result was positive\n",
      "</>\n",
      "Model inputs of address with \"city + state\" as separate inputs from a dataset would violate which accuracy guideline?\n",
      "A: Domain expertise\n",
      "B: Objective summarization\n",
      "C: First principles\n",
      "D: No correlating data\n",
      "---\n",
      "No correlating data\n",
      "</>\n",
      "Once a dataset has been cleaned, which accuracy guideline ensures your model is looking at the problem correctly? \n",
      "A: Objective summarization\n",
      "B: Domain expertise\n",
      "C: First principles\n",
      "D: Dataset verification\n",
      "---\n",
      "Domain expertise\n",
      "</>\n",
      "A good example of cultural reflection in training data is \n",
      "A: A model fails to recognize cultural differences due to incorrect attributes\n",
      "B: A predictive model incorporates training data from a variety of sources\n",
      "C: A model selects for one demographic less often because of their historical representation \n",
      "D: An image recognition model selects one face over another based on sample data\n",
      "---\n",
      "A model selects for one demographic less often because of their historical representation \n",
      "</>\n",
      "A good example of empirical reflection in training data is\n",
      "A: An image recognition model cannot tell a difference between a photo of a dog and a photo of a photo of a dog\n",
      "B: An image recognition model selects one face over another based on sample data\n",
      "C: A true positive result that defies the training data set\n",
      "D: A model fails to recognize cultural differences due to incorrect attributes\n",
      "---\n",
      "An image recognition model cannot tell a difference between a photo of a dog and a photo of a photo of a dog\n",
      "</>\n",
      "A training set based on feeding 60% of data, validating on 20% of data, and then designing multiple tests for the remaining 20% of data is referred to as an \n",
      "A: False positive set\n",
      "B: Optimized training set\n",
      "C: Predictive training set\n",
      "D: Classic training set\n",
      "---\n",
      "Optimized training set\n",
      "</>\n",
      "Our goals for building an ethical predictive model include making sure the results are \n",
      "A: Precise, methodical, ethical\n",
      "B: Accurate, precise, fair\n",
      "C: Precise, explainable, predictable\n",
      "D: Accurate, fair and explainable\n",
      "---\n",
      "Accurate, fair and explainable\n",
      "</>\n",
      "Unknown unknowns refer to\n",
      "A: Facing unknown empirical data with an incomplete dataset\n",
      "B: An uncertainty of how the data is gathered\n",
      "C: Being unsure about the morals of the research team\n",
      "D: Lack of explainability and what a model is actually looking at to make it's prediction\n",
      "---\n",
      "Lack of explainability and what a model is actually looking at to make it's prediction\n",
      "</>\n",
      "Narrow ai (ani) is defined as a specific type of artificial intelligence in which a technology outperforms humans in some defined task. \n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "An ethical, evolved predictive model would need to mimic a researcher's ability to _________________\n",
      "A: Scrub data\n",
      "B: Self-learn\n",
      "C: Parse through datasets\n",
      "D: Eliminate bias\n",
      "---\n",
      "Eliminate bias\n",
      "</>\n",
      "The second evolution of decision-making ai would enable\n",
      "A: Predictive models to drive cars\n",
      "B: Predictive models to start companies\n",
      "C: Predictive models to decide war strategy\n",
      "D: Predictive models to approve loans\n",
      "---\n",
      "Predictive models to decide war strategy\n",
      "</>\n",
      "Researchers believe that a general-purpose ai must be available to as many as possible, making it similar to a ________\n",
      "A: Government program\n",
      "B: Utility\n",
      "C: Tax plan\n",
      "D: System of money\n",
      "---\n",
      "Utility\n",
      "</>\n",
      "A perverse instantiation is an unintended negative outcome of programming a  goal that is too specific given to general intelligence\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "For-profit colleges tend to use predictive models\n",
      "A: To accelerate their research departments\n",
      "B: To see which candidates are most likely to receive government loans. \n",
      "C: To see which applicants are most likely to graduate\n",
      "D: To evaluate the standards of their professors\n",
      "---\n",
      "To see which candidates are most likely to receive government loans. \n",
      "</>\n",
      "For a model to clean, parse, and self-train it's own dataset while remaining impartial, the model needs\n",
      "A: A test for recency bias\n",
      "B: 10x the amount of data\n",
      "C: A list of bias and domain tests to run and adjust for\n",
      "D: More powerful computing algorithms to auto-scrub data\n",
      "---\n",
      "A list of bias and domain tests to run and adjust for\n",
      "</>\n",
      "For a model to make decisions that involve human life, the model needs\n",
      "A: A list of bias tests to run against possible wrong outcomes\n",
      "B: Enough computing power to make correct predictions 100% of the time\n",
      "C: A moral code of reasoning and priorities\n",
      "D: Programmed reflexive decision making ability \n",
      "---\n",
      "A moral code of reasoning and priorities\n",
      "</>\n",
      "A type of artificial intelligence that outperforms humans in some defined task is known as \n",
      "A: Special ai\n",
      "B: General ai\n",
      "C: Aei\n",
      "D: Narrow ai\n",
      "---\n",
      "Narrow ai\n",
      "</>\n",
      "A type of artificial intelligence that outperforms humans in all tasks is known as  \n",
      "A: Encompassing ai\n",
      "B: Outwit ai\n",
      "C: General ai\n",
      "D: Specific ai\n",
      "---\n",
      "General ai\n",
      "</>\n",
      "In 2019, ____% of equity-futures and cash-equity trades were executed by algorithms\n",
      "A: 20-30%\n",
      "B: 80-90%\n",
      "C: 11-17%\n",
      "D: 1-5%\n",
      "---\n",
      "80-90%\n",
      "</>\n",
      "The optimistic view of general ai could be accurately summarized as ai as a ____\n",
      "A: Peace-keeping tool\n",
      "B: Human right\n",
      "C: Weapon\n",
      "D: Utility\n",
      "---\n",
      "Utility\n",
      "</>\n",
      "The pessimist view of general ai references a scenario in which advancement is _____\n",
      "A: Creating ai for all governments\n",
      "B: Impossible\n",
      "C: Winner take all \n",
      "D: A potential extinction event\n",
      "---\n",
      "Winner take all \n",
      "</>\n",
      "An ethical general purpose ai must _____ while not harming the safety of humanity\n",
      "A: Keep those in power responsible\n",
      "B: Be 100% accurate\n",
      "C: Benefit as many people as possible\n",
      "D: Not enact hate\n",
      "---\n",
      "Benefit as many people as possible\n",
      "</>\n",
      "\"companies have an obligation to their shareholders\" is part of a view that sees artificial intelligence as \n",
      "A: An overall good for humanity, no matter the consequences\n",
      "B: A harmful tool that will bring about the end of capitalism\n",
      "C: Just another tool that accelerates research, like online advertising\n",
      "D: A gimmick for enterprises, unless general intelligence is achieved\n",
      "---\n",
      "Just another tool that accelerates research, like online advertising\n",
      "</>\n",
      "An unintended negative outcome of programming a broad goal into  general intelligence is known as \n",
      "A: Perverse instantiation\n",
      "B: Artificial sanctification\n",
      "C: An ethical dilemma \n",
      "D: An enduring output\n",
      "---\n",
      "Perverse instantiation\n",
      "</>\n",
      "True or false: the definition of fairness is \"just treatment without bias and contempt\"\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "Statistical parity as a fairness goal makes the most sense when \n",
      "A: Distributing randomly, ex. tickets\n",
      "B: Distributing by merit, ex. loans\n",
      "C: Distributing by gender, ex. tickets\n",
      "D: Distributing by error rate, ex. loans\n",
      "---\n",
      "Distributing randomly, ex. tickets\n",
      "</>\n",
      "Error rate parity means an equal chance of \n",
      "A: Outcomes for each group\n",
      "B: Prediction rate for each group\n",
      "C: Mistakes made for each group\n",
      "D: Approval for each group\n",
      "---\n",
      "Mistakes made for each group\n",
      "</>\n",
      "If you know one group is misrepresented in merit by training data, one way to ensure fairness is to\n",
      "A: Delete that group's data from the dataset\n",
      "B: Protect a different group\n",
      "C: Create a separate threshold for that group\n",
      "D: Reboot the training data\n",
      "---\n",
      "Create a separate threshold for that group\n",
      "</>\n",
      "True or false: fairness in machine learning cannot protect all individuals within protected groups from harm\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "Our goal in machine learning fairness is to minimize _______ as long as _______ is obtained \n",
      "A: Accuracy issues, unfairness\n",
      "B: Unfairness, equality\n",
      "C: Error rates, parity\n",
      "D: Equality, error rates\n",
      "---\n",
      "Error rates, parity\n",
      "</>\n",
      "Fairness is best defined as just treatment without __________\n",
      "A: Prejudice and favoritism \n",
      "B: Discrimination and prejudice\n",
      "C: Bias and contempt\n",
      "D: Favoritism or discrimination\n",
      "---\n",
      "Favoritism or discrimination\n",
      "</>\n",
      "Which type of fairness would make sense when dividing tickets evenly between groups? \n",
      "A: Equality of prediction rate\n",
      "B: Statistical parity\n",
      "C: Equality of false positives\n",
      "D: Error rate parity\n",
      "---\n",
      "Statistical parity\n",
      "</>\n",
      "Which type of fairness fails to address merit while maintaining accuracy? \n",
      "A: Error rate parity\n",
      "B: Statistical parity\n",
      "C: Equality of prediction rate\n",
      "D: Equality of false positives\n",
      "---\n",
      "Statistical parity\n",
      "</>\n",
      "A model that prioritizes equality on the outputs uses \n",
      "A: Equality of prediction rate\n",
      "B: Equality of assignment rate\n",
      "C: Error rate parity\n",
      "D: Statistical parity\n",
      "---\n",
      "Error rate parity\n",
      "</>\n",
      "Fairness in machine learning can protect groups from bias, but can still harm \n",
      "A: Individuals within those groups\n",
      "B: Future models\n",
      "C: Researchers\n",
      "D: Training datasets\n",
      "---\n",
      "Individuals within those groups\n",
      "</>\n",
      "A goal of a fair model's accuracy standards is to\n",
      "A: Minimize the quality metrics as long as the quantity metrics aren't affected\n",
      "B: Minimize the error rate as long as the training data isn't affected\n",
      "C: Minimize the error rate as long as parity is obtained\n",
      "D: Minimize the fairness score as long as the error rate isn't affected\n",
      "---\n",
      "Minimize the error rate as long as parity is obtained\n",
      "</>\n",
      "A model that makes more mistakes by moving its decision threshold down 40% of its worthiness metric will be potentially \n",
      "A: Fairer but less accurate\n",
      "B: Less fair but more accurate\n",
      "C: More accurate and fairer\n",
      "D: Less accurate and less fair\n",
      "---\n",
      "Fairer but less accurate\n",
      "</>\n",
      "If one group comprises the majority of the training data, they will skew the dataset and give the model \n",
      "A: More fairness for that group\n",
      "B: Less fairness for that group\n",
      "C: More confidence about that group\n",
      "D: Less confidence about that group\n",
      "---\n",
      "More confidence about that group\n",
      "</>\n",
      "If we know one group's worthiness score has been artificially inflated, one solution for fairness is to \n",
      "A: Remove that group from the dataset\n",
      "B: Add the inflation to the other data \n",
      "C: Balance the error rate by prioritizing the other group\n",
      "D: Creating separate decision thresholds for each group\n",
      "---\n",
      "Creating separate decision thresholds for each group\n",
      "</>\n",
      "An unfair model will by nature\n",
      "A: Try to balance groups automatically\n",
      "B: Optimize for making the most errors\n",
      "C: Optimize for making the fewest mistakes\n",
      "D: Optimize for making the fewest decisions\n",
      "---\n",
      "Optimize for making the fewest mistakes\n",
      "</>\n",
      "True or false: it is practical to protect all possible subgroups in predictive modeling\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "In machine learning, a pareto curve helps us\n",
      "A: Highlight the inequality in our model\n",
      "B: Pick an optimal threshold for accuracy and error rate\n",
      "C: Pick an optimal tradeoff between fairness and accuracy\n",
      "D: Select which model will give the best results\n",
      "---\n",
      "Pick an optimal tradeoff between fairness and accuracy\n",
      "</>\n",
      "True or false: a blind attribute model protects group fairness by not including group membership in predictions\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "An adversarial algorithm is _______________ to identify weaknesses in black box models\n",
      "A: Given no data\n",
      "B: Purposefully biased\n",
      "C: Purposefully fair\n",
      "D: Trained with large datasets\n",
      "---\n",
      "Purposefully biased\n",
      "</>\n",
      "True or false: our analysis revealed that word2vec is not a black box model\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "Which step in the fairness process would be most appropriate to introduce an auditing model? \n",
      "A: Sub-processing\n",
      "B: Pre-processing\n",
      "C: Post-processing\n",
      "D: In-processing\n",
      "---\n",
      "In-processing\n",
      "</>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A state where resources cannot be reallocated to make one individual better off without making at least one individual worse off is known as a\n",
      "A: Pareto efficiency\n",
      "B: Prisoner's dilemma\n",
      "C: Boron letter\n",
      "D: Aggregate curve\n",
      "---\n",
      "Pareto efficiency\n",
      "</>\n",
      "In machine learning, what do we plot on the x,y axis to determine a pareto curve?\n",
      "A: Error rate, rejection rate\n",
      "B: Rejection rate, false-positive rate\n",
      "C: Rejection rate, subgroup fairness rate\n",
      "D: Error rate, true positive rate\n",
      "---\n",
      "Error rate, rejection rate\n",
      "</>\n",
      "Why is it impractical to protect all possible subgroups in predictive models?\n",
      "A: Individuals do not need protection from predictive models\n",
      "B: Fairness scores won't be high enough to be reasonable\n",
      "C: Accuracy will be lowered beyond a reasonable rate\n",
      "D: There won't be enough data to reflect each subgroup\n",
      "---\n",
      "Accuracy will be lowered beyond a reasonable rate\n",
      "</>\n",
      "A model that equalizes the number of mistakes it makes for each subgroup to reduce harm is deciding on \n",
      "A: Equality of false negatives\n",
      "B: Equality of training data\n",
      "C: Equality of prediction bias\n",
      "D: Equality of true outcomes\n",
      "---\n",
      "Equality of false negatives\n",
      "</>\n",
      "A _________ model can still be unfair even though it won't explicitly know which groups are being inputted into the system\n",
      "A: Single attribute\n",
      "B: Biased training\n",
      "C: Blind attribute\n",
      "D: False-negative optimized\n",
      "---\n",
      "Blind attribute\n",
      "</>\n",
      "What tools do researchers have to evaluate the fairness of existing black box models?\n",
      "A: Evaluate inputs, evaluate data\n",
      "B: Change training data, evaluate outputs\n",
      "C: Change inputs, evaluate outputs\n",
      "D: Change inputs, evaluate training data\n",
      "---\n",
      "Change inputs, evaluate outputs\n",
      "</>\n",
      "A \"purposefully biased\" algorithm used to identify unfair attributes is known as \n",
      "A: A predictive model\n",
      "B: An aggregate algorithm\n",
      "C: A discriminatory algorithm\n",
      "D: An adversarial algorithm\n",
      "---\n",
      "An adversarial algorithm\n",
      "</>\n",
      "In presenting an audit report, a researcher would\n",
      "A: Prevent the model from launching\n",
      "B: De-bias the results\n",
      "C: Score the weight of input attributes on output\n",
      "D: Re-train the model\n",
      "---\n",
      "Score the weight of input attributes on output\n",
      "</>\n",
      "In fixing the word2vec model, we have an advantage over a traditional black box model in that\n",
      "A: We can decide which inputs to use\n",
      "B: We have access to the training data\n",
      "C: We can see the decision-making model\n",
      "D: We can generate a fairness score\n",
      "---\n",
      "We have access to the training data\n",
      "</>\n",
      "An auditing model is an example of a ______ bias mitigation method\n",
      "A: Sub-processing\n",
      "B: Pre-processing\n",
      "C: Post-processing\n",
      "D: In-processing\n",
      "---\n",
      "In-processing\n",
      "</>\n",
      "True or false: the no free lunch theorem states that we cannot have fair models without giving up something else\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "Which of the following is a good example of sample bias?\n",
      "A: Your model is trained to recognize pets, but you only give it photos of dogs\n",
      "B: Your model is trained to avoid bias, but it contains no samples of that bias\n",
      "C: Your model is broken because it cannot sample the right attribute\n",
      "D: Your model is designed to give loans to those who need it, but it is trained with unfair data\n",
      "---\n",
      "Your model is trained to recognize pets, but you only give it photos of dogs\n",
      "</>\n",
      "When cleaning/parsing data removes a potentially important attribute, that is referred to as \n",
      "A: Confirmation bias\n",
      "B: Automation bias\n",
      "C: Exclusion bias\n",
      "D: Observer bias\n",
      "---\n",
      "Exclusion bias\n",
      "</>\n",
      "Labeling outputs made by predictive models can avoid which feedback issue?\n",
      "A: Predictive loop bias\n",
      "B: Fairness score bias\n",
      "C: Re-training bias\n",
      "D: Sample bias\n",
      "---\n",
      "Re-training bias\n",
      "</>\n",
      "True or false: recommendation engines are not as susceptible to feedback bias due to their constraints on data inputs\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "A \"best response\" in game theory is when\n",
      "A: A user chooses the least fair option\n",
      "B: A user leaves a group setting to pursue individual goals\n",
      "C: A user has no choice but to follow the group dynamic to benefit\n",
      "D: A user comes up with the best response from the group\n",
      "---\n",
      "A user has no choice but to follow the group dynamic to benefit\n",
      "</>\n",
      "The __________ theorem states that all models have the same error rate when averaged over all possible data generating distributions.\n",
      "A: Pareto efficiency\n",
      "B: Zero handouts\n",
      "C: No free lunch\n",
      "D: Biased aggregate\n",
      "---\n",
      "No free lunch\n",
      "</>\n",
      "As a cognitive bias, humans see lack of context/meaning around a piece of information and tend to \n",
      "A: Fill in gaps with existing knowledge\n",
      "B: Seek authority figures\n",
      "C: Disagree with their previous beliefs\n",
      "D: Use biased sources of research\n",
      "---\n",
      "Fill in gaps with existing knowledge\n",
      "</>\n",
      "When your collected data doesn't accurately reflect the full environment, you're experiencing\n",
      "A: Exclusion bias\n",
      "B: Prejudice bias\n",
      "C: Sample bias\n",
      "D: Observer bias\n",
      "---\n",
      "Sample bias\n",
      "</>\n",
      "The tendency to only seek attributes in existing collected data is known as \n",
      "A: Exclusion bias\n",
      "B: Observer bias\n",
      "C: Prejudice bias\n",
      "D: Availability bias\n",
      "---\n",
      "Availability bias\n",
      "</>\n",
      "An example of automation bias is\n",
      "A: Using new data over existing data\n",
      "B: Using parsed twitter data over parsed facebook data\n",
      "C: Using scraped twitter data over survey data\n",
      "D: Using biased survey data instead of parsed survey data\n",
      "---\n",
      "Using scraped twitter data over survey data\n",
      "</>\n",
      "A ________ is when a model is validated by it's own influence on predictions\n",
      "A: Dataset scrub\n",
      "B: False prediction set\n",
      "C: Self-fulfilling prediction\n",
      "D: Training set error\n",
      "---\n",
      "Self-fulfilling prediction\n",
      "</>\n",
      "One way to avoid feedback loops in machine learning is to \n",
      "A: Scrub datasets after each decision\n",
      "B: Investigate exclusion bias\n",
      "C: Destroy previous training data\n",
      "D: Label outputs to prevent re-training bias\n",
      "---\n",
      "Label outputs to prevent re-training bias\n",
      "</>\n",
      "Dating algorithms become biased mostly through offering users _________\n",
      "A: Different ways to match with users\n",
      "B: Unlimited matches per day\n",
      "C: Collaborative filtering\n",
      "D: Access to separate data models\n",
      "---\n",
      "Collaborative filtering\n",
      "</>\n",
      "Predictive loops in marketplace models like dating apps are especially susceptible to bias due to\n",
      "A: Marketplace forces\n",
      "B: Fairness quotients\n",
      "C: Short feedback cycles\n",
      "D: Engagement levels\n",
      "---\n",
      "Short feedback cycles\n",
      "</>\n",
      "Game theory states that outcomes that are best for _____ can be obscured by outcomes best for ______\n",
      "A: The group, the individual\n",
      "B: The dataset, the model\n",
      "C: The model, the individual\n",
      "D: The group, the dataset\n",
      "---\n",
      "The group, the individual\n",
      "</>\n",
      "A dataset column that cannot directly identify, like zip code, is called a\n",
      "A: Quasi-identifiable column\n",
      "B: Sensitive column\n",
      "C: Non-sensitive column\n",
      "D: Personally identifiable column\n",
      "---\n",
      "Quasi-identifiable column\n",
      "</>\n",
      "True or false: k-anonymity protects users from all privacy violations\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "Which of the following is an example of partially obfuscated data?\n",
      "A: 60-70 years old\n",
      "B: Had a stroke\n",
      "C: 65 years old\n",
      "D: 94103\n",
      "---\n",
      "60-70 years old\n",
      "</>\n",
      "True or false: a non-sensitive column can endanger user privacy\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "Why is it important to limit precise outputs in a predictive model? \n",
      "A: It removes the datasets in danger of violating privacy\n",
      "B: It can help users make better decisions\n",
      "C: It can help limit adversarial attacks \n",
      "D: It limits the ability of researchers\n",
      "---\n",
      "It can help limit adversarial attacks \n",
      "</>\n",
      "Where is a perturbed input used?\n",
      "A: When building a model\n",
      "B: To increase public dataset awareness\n",
      "C: To identify an error in a machine learning model\n",
      "D: To observe differing outputs\n",
      "---\n",
      "To observe differing outputs\n",
      "</>\n",
      "A dataset attribute that is not identifiable but constitutes data about the individual that needs to be protected is known as a \n",
      "A: Quasi-identifier\n",
      "B: Explicitly private column \n",
      "C: Non-sensitive column\n",
      "D: Sensitive column\n",
      "---\n",
      "Sensitive column\n",
      "</>\n",
      "K-anonymity in a dataset is achieved when each individual cannot be\n",
      "A: Harmed from datasets with k individuals belonging to the sensitive class\n",
      "B: Hidden from a quasi-identifier column as long as k individuals belong\n",
      "C: Reidentified in k datasets\n",
      "D: Distinguished from at least k individuals who are also in the dataset\n",
      "---\n",
      "Distinguished from at least k individuals who are also in the dataset\n",
      "</>\n",
      "A major downside to k-anonymity is that re-identification is possible with\n",
      "A: Multiple datasets\n",
      "B: Sensitive columns\n",
      "C: Database leaks\n",
      "D: Expanding k values\n",
      "---\n",
      "Multiple datasets\n",
      "</>\n",
      "A hospital dataset protects whether an individual has had either a stroke, heart attack, or staph infection. the individual may still be harmed via dataset\n",
      "A: Group inclusion\n",
      "B: Security issues\n",
      "C: K-anonymity\n",
      "D: Privacy columns\n",
      "---\n",
      "Group inclusion\n",
      "</>\n",
      "A non-sensitive column may become sensitive or even identifiable when viewed through the lens of \n",
      "A: Community privacy violations\n",
      "B: Societal privacy violations\n",
      "C: Algorithmic privacy violations\n",
      "D: Human privacy violations\n",
      "---\n",
      "Algorithmic privacy violations\n",
      "</>\n",
      "An example of a public dataset at risk of an algorithmic privacy violation is the \n",
      "A: Google search view dataset\n",
      "B: Fidelity bank lending dataset\n",
      "C: Stanford hospital dataset\n",
      "D: Google maps satellite view dataset\n",
      "---\n",
      "Google maps satellite view dataset\n",
      "</>\n",
      "The netflix prize privacy scandal is an example of reidentification through\n",
      "A: Multiple datasets\n",
      "B: Database leaks\n",
      "C: K-anonymity\n",
      "D: Sensitive columns\n",
      "---\n",
      "Multiple datasets\n",
      "</>\n",
      "The nature of a predictive model may reveal ___________\n",
      "A: The researchers behind it\n",
      "B: The data it is trained on\n",
      "C: The algorithm's bias\n",
      "D: The ethics of the individual inputs\n",
      "---\n",
      "The data it is trained on\n",
      "</>\n",
      "An adversarial model relies on using __________ to observe different outputs\n",
      "A: Algorithm detection\n",
      "B: Perturbed inputs\n",
      "C: Sensitive columns\n",
      "D: Pressure inputs\n",
      "---\n",
      "Perturbed inputs\n",
      "</>\n",
      "One way to counter a potential adversarial algorithm is by \n",
      "A: Changing the datasets\n",
      "B: Limiting precise outputs\n",
      "C: Banning model updates\n",
      "D: Improving model transparency\n",
      "---\n",
      "Limiting precise outputs\n",
      "</>\n",
      "Which of the following is not a recommended security practice in machine learning?\n",
      "A: Create a chain of command\n",
      "B: Ensure that all team members/stakeholders have  a basic understanding of security\n",
      "C: Perform threat modeling\n",
      "D: Enact a sound data governance structure\n",
      "---\n",
      "Create a chain of command\n",
      "</>\n",
      "True or false: data minimization is the concept of shrinking datasets to only what is required to fulfill a general purpose \n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "A model working at 95% with 100k rows of data and 97% with 500k rows is an example of \n",
      "A: Dataset limitations\n",
      "B: Accuracy issues\n",
      "C: Diminishing returns \n",
      "D: Sensitive columns\n",
      "---\n",
      "Diminishing returns \n",
      "</>\n",
      "True or false: global differential privacy is added after data is collected\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "Randomized response in local differential privacy gives its users which of the following?  \n",
      "A: Plausible deniability\n",
      "B: Global privacy\n",
      "C: K-anonymity\n",
      "D: Fairness guarantees\n",
      "---\n",
      "Plausible deniability\n",
      "</>\n",
      "In reverse-engineering a double coin flip differential model, what would be the amount of falsified \"yes/no\" responses in our dataset?\n",
      "A: 75%\n",
      "B: 50%\n",
      "C: 25%\n",
      "D: 100%\n",
      "---\n",
      "25%\n",
      "</>\n",
      "Which of the following is a recommended security practice for machine learning datasets?\n",
      "A: Ensure your team is full of different perspectives\n",
      "B: Perform threat modeling with beneficial algorithms\n",
      "C: Create a chain of command\n",
      "D: Enact a sound data governance structure\n",
      "---\n",
      "Enact a sound data governance structure\n",
      "</>\n",
      "The data minimization principle requires that you limit data collection to only what is __________\n",
      "A: Necessary for differential privacy\n",
      "B: Needed to have k-anonymity\n",
      "C: Required to fulfill a specific purpose \n",
      "D: Optional for a chain of command\n",
      "---\n",
      "Required to fulfill a specific purpose \n",
      "</>\n",
      "Delete unused data __________ is a method of data minimization\n",
      "A: Early and often\n",
      "B: Before modeling\n",
      "C: After fairness preparations\n",
      "D: Before threat modeling\n",
      "---\n",
      "Early and often\n",
      "</>\n",
      "Gdpr states that \"personal data shall be adequate, relevant and __________ in relation to the purpose or purposes for which they are processed.\"\n",
      "A: Not excessive\n",
      "B: Highly specified\n",
      "C: Broadly applicable\n",
      "D: Thoroughly vetted\n",
      "---\n",
      "Not excessive\n",
      "</>\n",
      "What can be learned from a predictive model should not change if the _________ is either included or excluded in the training set\n",
      "A: Dataset filter\n",
      "B: Individual's data\n",
      "C: Model fairness score\n",
      "D: Biased dataset\n",
      "---\n",
      "Individual's data\n",
      "</>\n",
      "Differential privacy works by adding what to a dataset? \n",
      "A: Filters\n",
      "B: Fairness scoring\n",
      "C: K-anonymity\n",
      "D: Noise\n",
      "---\n",
      "Noise\n",
      "</>\n",
      "If a coin is flipped, which of the following would ensure \"yes/no\" data is private while still remaining useful? \n",
      "A: Heads for true answer, tails for random answer\n",
      "B: Heads for yes, tails for no\n",
      "C: Heads for fake answer, tails for true answer\n",
      "D: Heads for random answer, tails for yes\n",
      "---\n",
      "Heads for true answer, tails for random answer\n",
      "</>\n",
      "At which level of differential privacy is the outcome secured from even the people collecting answers?\n",
      "A: K-anonymous\n",
      "B: Premium\n",
      "C: Local\n",
      "D: Global\n",
      "---\n",
      "Local\n",
      "</>\n",
      "Plausible deniability refers to the ability of the individual to ______\n",
      "A: Remove themselves from a machine learning dataset\n",
      "B: Claim their score was randomized response\n",
      "C: Express their participation in a study\n",
      "D: Doubt the results of a study\n",
      "---\n",
      "Claim their score was randomized response\n",
      "</>\n",
      "In reverse-engineering a double coin flip differential model, what would be the amount of truthful \"yes/no\" responses in our dataset?\n",
      "A: 50%\n",
      "B: 25%\n",
      "C: 100%\n",
      "D: 75%\n",
      "---\n",
      "75%\n",
      "</>\n",
      "True or false: a glass box model is traditionally preferred by businesses\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "False\n",
      "</>\n",
      "An ethical model will be more fair and explainable, at the expense of ________\n",
      "A: Recruiting\n",
      "B: Accuracy\n",
      "C: Dataset collection\n",
      "D: Privacy\n",
      "---\n",
      "Accuracy\n",
      "</>\n",
      "Explainable models can provide _________, which look very similar to auditing models\n",
      "A: Model renderings\n",
      "B: Dataset fairness graphs\n",
      "C: Attribute weight charts\n",
      "D: Privacy protection\n",
      "---\n",
      "Attribute weight charts\n",
      "</>\n",
      "Which of the following is a strategy to improve group outcomes using game theory? \n",
      "A: Fairness-enforcing methods\n",
      "B: Explainable recommendation systems\n",
      "C: Privacy-enforcing algorithms\n",
      "D: Ethical predictive sets\n",
      "---\n",
      "Explainable recommendation systems\n",
      "</>\n",
      "True or false: glass box models allow agents to re-run them\n",
      "A: True\n",
      "B: False\n",
      "---\n",
      "True\n",
      "</>\n",
      "A differentially private search algorithm would add some ______ at the expense of _______ \n",
      "A: Noise, privacy\n",
      "B: Accuracy, noise\n",
      "C: Noise, accuracy\n",
      "D: Data points, privacy\n",
      "---\n",
      "Noise, accuracy\n",
      "</>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical models are _________\n",
      "A: Precise, explainable, and fair\n",
      "B: Precise, explainable, and private\n",
      "C: Accurate, methodical, and fair\n",
      "D: Accurate, explainable, and fair\n",
      "---\n",
      "Accurate, explainable, and fair\n",
      "</>\n",
      "A _____ model is preferred by businesses because they see less competition as a benefit\n",
      "A: Black box\n",
      "B: Fair\n",
      "C: Ethical\n",
      "D: Precise\n",
      "---\n",
      "Black box\n",
      "</>\n",
      "One benefit of an explainable model is\n",
      "A: Privacy is preserved\n",
      "B: Recruiting leverage\n",
      "C: The model is more accurate\n",
      "D: Less competition\n",
      "---\n",
      "Recruiting leverage\n",
      "</>\n",
      "The strava dataset example illustrates that while differential privacy can protect individuals, it can still harm _______\n",
      "A: Certain individuals\n",
      "B: Those outside the dataset\n",
      "C: Groups\n",
      "D: Researchers\n",
      "---\n",
      "Groups\n",
      "</>\n",
      "The explainable ai movement states that cooperation between agents, in this case, algorithms and humans, depends on which of the following?\n",
      "A: Trust\n",
      "B: Privacy\n",
      "C: Human rights\n",
      "D: Competition\n",
      "---\n",
      "Trust\n",
      "</>\n",
      "A benefit of glass-box models is that if an attribute is skewing the fairness of a decision, a human agent may choose to\n",
      "A: Alert the team\n",
      "B: Trust it\n",
      "C: Ignore it\n",
      "D: Re-run the algorithm without it\n",
      "---\n",
      "Re-run the algorithm without it\n",
      "</>\n",
      "_____ algorithms are a challenge to explainable ai, as their complexity makes it difficult to weigh attribute importance\n",
      "A: Explainable\n",
      "B: Ethical\n",
      "C: Deep learning\n",
      "D: Malicious\n",
      "---\n",
      "Deep learning\n",
      "</>\n",
      "When used in recommendation engines, explainable algorithms can help answer the question of ____\n",
      "A: How?\n",
      "B: Who?\n",
      "C: Why?\n",
      "D: What?\n",
      "---\n",
      "Why?\n",
      "</>\n",
      "Group outcomes being improved through explainable recommendation systems is an example of using _______ to modify outcomes\n",
      "A: Competition\n",
      "B: Game theory\n",
      "C: Privacy\n",
      "D: Government\n",
      "---\n",
      "Game theory\n",
      "</>\n",
      "A major limitation of using explainable, adjustable algorithms is that users tend to make _____ decisions\n",
      "A: Malicious\n",
      "B: Selfish\n",
      "C: Socially conscious\n",
      "D: Private\n",
      "---\n",
      "Selfish\n",
      "</>\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open(file,\n",
    "              encoding='utf-8') as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "        extract_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd81fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
